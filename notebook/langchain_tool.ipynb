{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source ../.venv/bin/activate\n",
    "! python -m pip install langchain_community langchain-openai langchain-anthropic scikit-learn bs4 pandas pyarrow matplotlib lxml langgraph \"mcp[cli]\" python-dotenv\n",
    " \n",
    "\n",
    "\n",
    "# Import and inject truststore into the SSL module \n",
    "import truststore\n",
    "truststore.inject_into_ssl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Azure Openai and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "import os\n",
    "\n",
    " \n",
    "def get_llm():\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=\"gpt-4o-mini\", \n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=\"2024-12-01-preview\", \n",
    "        #verbose=True\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "def get_embeddings():\n",
    "    embeddings = AzureOpenAIEmbeddings( \n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "        #api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def test_embedding_llm():\n",
    "    llm = get_llm()\n",
    "    response = llm.invoke(\"What is 1+1\")\n",
    "    print(response)\n",
    "\n",
    "    embeddings = get_embeddings()\n",
    "    embedding_vector = embeddings.embed_query(\"hello!\")\n",
    "    # Print embedding stats\n",
    "    print(f\"\\nEmbedding vector length: {len(embedding_vector)}\")\n",
    "    print(f\"First 5 values: {embedding_vector[:5]}\") \n",
    "    \n",
    "\n",
    "\n",
    "test_embedding_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to build the local context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "\n",
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))\n",
    "\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Target the main article content for Paychex documentation \n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "    \n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "    \n",
    "    return content\n",
    "\n",
    "def load_paychex_docs():\n",
    "    \"\"\"\n",
    "    Load Paychex documentation from the official website.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the Paychex website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading Paychex documentation...\")\n",
    "\n",
    "    # Load the documentation \n",
    "    urls = [\"https://www.paychex.com/payroll\", \n",
    "    ] \n",
    "\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=5,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from Paychex documentation.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i+1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "    \n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "    \n",
    "    return docs, tokens_per_doc\n",
    "\n",
    "def save_llms_full(documents):\n",
    "    \"\"\" Save the documents to a file \"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"llms_full.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get('source', 'Unknown URL')\n",
    "            \n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n",
    "\n",
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "    \n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000,  \n",
    "        chunk_overlap=500  \n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "    \n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "    \n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "    \n",
    "    return split_docs\n",
    " \n",
    "\n",
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "    \n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "        \n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd()+\"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=get_embeddings(),\n",
    "        persist_path=persist_path   ,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "    \n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_paychex_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the documents\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m documents, tokens_per_doc = \u001b[43mload_paychex_docs\u001b[49m()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save the documents to a file\u001b[39;00m\n\u001b[32m      5\u001b[39m save_llms_full(documents)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_paychex_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_paychex_docs()\n",
    "\n",
    "# Save the documents to a file\n",
    "save_llms_full(documents)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = split_documents(documents)\n",
    "\n",
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "# Get relevant documents for the query\n",
    "query = \"What are the paychex services ?\"    \n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata['source'])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create tool To test the the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def paychex_website_query_tool(query: str):\n",
    "    \"\"\"\n",
    "    Query the Paychex website using a retriever.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to search the documentation with\n",
    "\n",
    "    Returns:\n",
    "        str: A str of the retrieved documents\n",
    "    \"\"\"\n",
    "    retriever = SKLearnVectorStore(\n",
    "    embedding=get_embeddings(), \n",
    "    persist_path=os.getcwd()+\"/sklearn_vectorstore.parquet\", \n",
    "    serializer=\"parquet\").as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    relevant_docs = retriever.invoke(query)\n",
    "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "    formatted_context = \"\\n\\n\".join([f\"==DOCUMENT {i+1}==\\n{doc.page_content}\" for i, doc in enumerate(relevant_docs)])\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Tool using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paychex_website_query_tool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Convert the function to OpenAI function format\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tools = [\u001b[43mpaychex_website_query_tool\u001b[49m]\n\u001b[32m     10\u001b[39m messages = [\n\u001b[32m     11\u001b[39m     SystemMessage(content=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant that can answer questions about the Paychex website.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     12\u001b[39m     HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat is paychex website in payroll?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m ]\n\u001b[32m     14\u001b[39m model_with_tools = get_llm().bind_tools(tools)\n",
      "\u001b[31mNameError\u001b[39m: name 'paychex_website_query_tool' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "\n",
    "# Convert the function to OpenAI function format\n",
    "tools = [paychex_website_query_tool]\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that can answer questions about the Paychex website.\"),\n",
    "    HumanMessage(content=\"What is paychex website in payroll?\")\n",
    "]\n",
    "model_with_tools = get_llm().bind_tools(tools)\n",
    "#response = get_llm().invoke(messages, functions=functions) \n",
    "response = model_with_tools.invoke(messages)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
